{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/distilgpt2/resolve/main/vocab.json from cache at C:\\Users\\tharh/.cache\\huggingface\\transformers\\55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "loading file https://huggingface.co/distilgpt2/resolve/main/merges.txt from cache at C:\\Users\\tharh/.cache\\huggingface\\transformers\\9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/distilgpt2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilgpt2/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at C:\\Users\\tharh/.cache\\huggingface\\transformers\\f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"distilgpt2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Adding <|startoftext|> to the vocabulary\n",
      "Adding <|pad|> to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at C:\\Users\\tharh/.cache\\huggingface\\transformers\\f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
      "Model config GPT2Config {\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin from cache at C:\\Users\\tharh/.cache\\huggingface\\transformers\\43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2', bos_token='<|startoftext|>',\n",
    "                                          eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"netflix_titles.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "description = df['description']\n",
    "listed_in =df[\"listed_in\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "['tv action & adventure', 'teen tv shows', 'cult movies', 'tv comedies', 'horror movies', 'tv thrillers', 'crime tv shows', 'lgbtq movies', 'stand-up comedy & talk shows', 'comedies', 'action & adventure', 'romantic movies', 'docuseries', 'stand-up comedy', 'music & musicals', 'tv dramas', 'thrillers', 'sports movies', 'anime series', 'korean tv shows', 'children & family movies', 'tv shows', 'international movies', 'classic & cult tv', 'tv mysteries', 'tv sci-fi & fantasy', 'tv horror', \"kids' tv\", 'british tv shows', 'anime features', 'romantic tv shows', 'classic movies', 'independent movies', 'movies', 'documentaries', 'spanish-language tv shows', 'faith & spirituality', 'reality tv', 'international tv shows', 'science & nature tv', 'dramas', 'sci-fi & fantasy']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "listed_in_arr = [ category.strip().lower().split(\",\") for category in listed_in]\n",
    "dist_listed = []\n",
    "temp =set()\n",
    "\n",
    "listed_in_new = []\n",
    "for category in listed_in_arr:\n",
    "    temp_list = []\n",
    "    for sub_cat in category:\n",
    "        temp.add(sub_cat.strip())\n",
    "        temp_list.append(sub_cat.strip())\n",
    "    listed_in_new.append(temp_list)\n",
    "dist_listed = list(temp)\n",
    "\n",
    "\n",
    "print(len(dist_listed))\n",
    "print(dist_listed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(listed_in_new))\n",
    "cat_list_newArr  =[]\n",
    "for cat_list in listed_in_new:\n",
    "    temp_arr = []\n",
    "    for cur_onehot in dist_listed:\n",
    "        issame = False\n",
    "        \n",
    "        for sub_cat_list in cat_list:\n",
    "            if cur_onehot==sub_cat_list:\n",
    "                issame=True\n",
    "                \n",
    "        if issame:\n",
    "            temp_arr.append(1)\n",
    "        else:\n",
    "            temp_arr.append(0)\n",
    "    cat_list_newArr.append(np.array(temp_arr))\n",
    "\n",
    "len(cat_list_newArr[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length :  62\n"
     ]
    }
   ],
   "source": [
    "num_of_flags = len(cat_list_newArr[0])\n",
    "max_length = max([len(tokenizer.encode(title.strip())) for title in description])\n",
    "print(\"max_length : \",max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8807"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensor of XY After Concatenation: tensor([5, 5, 5, 6, 6, 6])\n",
      "The tensor of YX After Concatenation: tensor([6, 6, 6, 5, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "X = torch.tensor([5, 5, 5])\n",
    "Y = torch.tensor([6, 6, 6])\n",
    "XY = torch.cat((X, Y), 0)\n",
    "YX = torch.cat((Y, X), 0)\n",
    "print('The tensor of XY After Concatenation:', XY)\n",
    "print('The tensor of YX After Concatenation:', YX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_amount =1000\n",
    "max_amount1= 1100\n",
    "\n",
    "train_description, test_description = description[:min_amount],description[min_amount:max_amount1]\n",
    "train_flags , test_flags = cat_list_newArr[:min_amount],cat_list_newArr[min_amount:max_amount1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42])\n",
      "torch.Size([62])\n",
      "torch.Size([104])\n",
      "tensor([50257,  1722,   607,  2988,  1474,    82,   262,   886,   286,   465,\n",
      "         1204,    11, 26479, 39700,   268,  5030,  9539,   465,  1918,   287,\n",
      "        47602,   290,   401,   605,  2842,   284,  1037,   606,  1111,  1986,\n",
      "          262, 13203,    13, 50256, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     1,     0,     0,     0,\n",
      "            0,     0,     0,     0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "X = torch.tensor( train_flags[i],dtype=torch.int32)\n",
    "encodings_dict = tokenizer('<|startoftext|>' + train_description[i] + '<|endoftext|>',\n",
    "                                       max_length=max_length, padding=\"max_length\")\n",
    "Y =  torch.tensor(encodings_dict['input_ids'],dtype=torch.int32)\n",
    "print(X.shape)\n",
    "print(Y.shape)                                  \n",
    "XY = torch.cat((Y,X), 0)\n",
    "print(XY.shape)\n",
    "print(XY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetflixDataset(Dataset):\n",
    "    def __init__(self, txt_list,flags, tokenizer, max_length,batch_size):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        for txt,cur_flag in zip(txt_list,flags):\n",
    "            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>',\n",
    "                                       max_length=max_length, padding=\"max_length\")\n",
    "            input_ids_X  = torch.tensor(encodings_dict['input_ids'])\n",
    "            cur_flag_tensor = torch.tensor(cur_flag)\n",
    "            input_ids_X_cur_flag = torch.cat((input_ids_X,cur_flag_tensor), 0)\n",
    "            self.input_ids.append(input_ids_X_cur_flag)\n",
    "\n",
    "\n",
    "            attention_mask_X  = torch.tensor(encodings_dict['attention_mask'])\n",
    "            attention_mask_X_cur_flag = torch.cat((attention_mask_X,cur_flag_tensor), 0)\n",
    "            self.attn_masks.append(attention_mask_X_cur_flag)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #input_ids_X_batches = self.input_ids[idx*self.batch_size : (idx+1)*self.batch_size]\n",
    "        #attn_masks_X_batches = self.attn_masks[idx*self.batch_size : (idx+1)*self.batch_size]\n",
    "        \n",
    "        return self.input_ids, self.attn_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NetflixDataset(train_description,train_flags, tokenizer, max_length=max_length,batch_size=8)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([50257,  1722,   607,  2988,  1474,    82,   262,   886,   286,   465,\n",
      "         1204,    11, 26479, 39700,   268,  5030,  9539,   465,  1918,   287,\n",
      "        47602,   290,   401,   605,  2842,   284,  1037,   606,  1111,  1986,\n",
      "          262, 13203,    13, 50256, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     1,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0])\n",
      "torch.Size([2, 104])\n",
      "tensor(50257)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for temp in train_dataset:\n",
    "    print(temp[0][0])\n",
    "    print(temp[1][0])\n",
    "    data = torch.stack([f[0] for f in temp])\n",
    "    print(data.shape)\n",
    "    data = torch.stack([f[1][0] for f in temp])\n",
    "    print(data[0])\n",
    "    break\n",
    "    \"\"\"\n",
    "    if len(temp[0])!=len(temp[1]) and len(temp[0])==62:\n",
    "        print(temp[0])\n",
    "        print(temp[1])\n",
    "    \n",
    "        break\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir='./results', num_train_epochs=5, logging_steps=100, save_steps=500,\n",
    "                                  per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "                                  warmup_steps=10, weight_decay=0.05, logging_dir='./logs', report_to = 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tharh\\anaconda3\\envs\\tharhtet\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 900\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d3aa93104f417487be75b36d4e31bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0407, 'learning_rate': 4.89977728285078e-05, 'epoch': 0.11}\n",
      "{'loss': 0.0682, 'learning_rate': 4.7884187082405344e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0212, 'learning_rate': 4.67706013363029e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0082, 'learning_rate': 4.565701559020045e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0035, 'learning_rate': 4.4543429844098e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0244, 'learning_rate': 4.342984409799555e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0016, 'learning_rate': 4.2316258351893094e-05, 'epoch': 0.78}\n",
      "{'loss': 0.0056, 'learning_rate': 4.120267260579065e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0111, 'learning_rate': 4.00890868596882e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0007, 'learning_rate': 3.897550111358575e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0021, 'learning_rate': 3.78619153674833e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0024, 'learning_rate': 3.674832962138085e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0001, 'learning_rate': 3.5634743875278396e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0001, 'learning_rate': 3.452115812917594e-05, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 3.34075723830735e-05, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 3.229398663697105e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0, 'learning_rate': 3.11804008908686e-05, 'epoch': 1.89}\n",
      "{'loss': 0.0001, 'learning_rate': 3.0066815144766146e-05, 'epoch': 2.0}\n",
      "{'loss': 0.0001, 'learning_rate': 2.89532293986637e-05, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 2.7839643652561248e-05, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 2.67260579064588e-05, 'epoch': 2.33}\n",
      "{'loss': 0.0, 'learning_rate': 2.561247216035635e-05, 'epoch': 2.44}\n",
      "{'loss': 0.002, 'learning_rate': 2.44988864142539e-05, 'epoch': 2.56}\n",
      "{'loss': 0.0, 'learning_rate': 2.338530066815145e-05, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 2.2271714922049e-05, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 2.1158129175946547e-05, 'epoch': 2.89}\n",
      "{'loss': 0.0, 'learning_rate': 2.00445434298441e-05, 'epoch': 3.0}\n",
      "{'loss': 0.0, 'learning_rate': 1.893095768374165e-05, 'epoch': 3.11}\n",
      "{'loss': 0.0, 'learning_rate': 1.7817371937639198e-05, 'epoch': 3.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-3000\n",
      "Configuration saved in ./results\\checkpoint-3000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.670378619153675e-05, 'epoch': 3.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-3000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.55902004454343e-05, 'epoch': 3.44}\n",
      "{'loss': 0.0011, 'learning_rate': 1.447661469933185e-05, 'epoch': 3.56}\n",
      "{'loss': 0.0, 'learning_rate': 1.33630289532294e-05, 'epoch': 3.67}\n",
      "{'loss': 0.0002, 'learning_rate': 1.224944320712695e-05, 'epoch': 3.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-3500\n",
      "Configuration saved in ./results\\checkpoint-3500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.11358574610245e-05, 'epoch': 3.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-3500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.002227171492205e-05, 'epoch': 4.0}\n",
      "{'loss': 0.0007, 'learning_rate': 8.908685968819599e-06, 'epoch': 4.11}\n",
      "{'loss': 0.0, 'learning_rate': 7.79510022271715e-06, 'epoch': 4.22}\n",
      "{'loss': 0.0008, 'learning_rate': 6.6815144766147e-06, 'epoch': 4.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-4000\n",
      "Configuration saved in ./results\\checkpoint-4000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 5.56792873051225e-06, 'epoch': 4.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-4000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 4.4543429844097995e-06, 'epoch': 4.56}\n",
      "{'loss': 0.0019, 'learning_rate': 3.34075723830735e-06, 'epoch': 4.67}\n",
      "{'loss': 0.0, 'learning_rate': 2.2271714922048998e-06, 'epoch': 4.78}\n",
      "{'loss': 0.0, 'learning_rate': 1.1135857461024499e-06, 'epoch': 4.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-4500\n",
      "Configuration saved in ./results\\checkpoint-4500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-4500\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 284.281, 'train_samples_per_second': 15.829, 'train_steps_per_second': 15.829, 'train_loss': 0.09328241164014778, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4500, training_loss=0.09328241164014778, metrics={'train_runtime': 284.281, 'train_samples_per_second': 15.829, 'train_steps_per_second': 15.829, 'train_loss': 0.09328241164014778, 'epoch': 5.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer(model=model, args=training_args, train_dataset=train_dataset, \n",
    "        eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0][0] for f in data]),\n",
    "                                                              'attention_mask': torch.stack([f[1][0] for f in data]),\n",
    "                                                              'labels': torch.stack([f[0][0] for f in data])}).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampel prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([50257,  1722,   607,  2988,  1474,    82,   262,   886,   286,   465,\n",
      "         1204,    11, 26479, 39700,   268,  5030,  9539,   465,  1918,   287,\n",
      "        47602,   290,   401,   605,  2842,   284,  1037,   606,  1111,  1986,\n",
      "          262, 13203,    13, 50256, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     1,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n",
      "104\n"
     ]
    }
   ],
   "source": [
    "inputS = torch.stack([f[0][0] for f in val_dataset])\n",
    "print(inputS[0])\n",
    "print(len(inputS[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input :  As her father nears the end of his life, filmmaker Kirsten Johnson stages his death in inventive and comical ways to help them both face the inevitable.\n",
      "Generated output :  As her father nears the end of his life, filmmaker Kirsten Johnson stages his death in inventive and comical ways to help them both face the inevitable.!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\"!!!!!!! - (:) # | TheMoral Election 2016 Voter Registration Results &Please send your ballot into an online form by Email This article is available on a liveET Your consent will be sent asapYourFriendKit Newsletter mailing list has been created using\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "X = torch.tensor( train_flags[i])\n",
    "encodings_dict = tokenizer('<|startoftext|>' + train_description[i] + '<|endoftext|>',\n",
    "                                       max_length=max_length, padding=\"max_length\")\n",
    "Y =  torch.tensor(encodings_dict['input_ids'])\n",
    "#print(X.shape)\n",
    "#print(Y.shape)                                  \n",
    "XY = torch.cat((Y,X), 0)\n",
    "\n",
    "sample_input = torch.reshape(XY,(1,-1)).cuda()\n",
    "#print(sample_input)\n",
    "#print(sample_input[0].shape)\n",
    "#print(len(sample_input[0]))\n",
    "\n",
    "sample_outputs = model.generate(sample_input,no_repeat_ngram_size = 1,num_beams=20, num_return_sequences=2,max_new_tokens=50)\n",
    "#print(len(sample_outputs))\n",
    "print(\"Input : \",train_description[i])\n",
    "print(\"Generated output : \",tokenizer.decode(sample_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_flags[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tv action & adventure', 'teen tv shows', 'cult movies', 'tv comedies', 'horror movies', 'tv thrillers', 'crime tv shows', 'lgbtq movies', 'stand-up comedy & talk shows', 'comedies', 'action & adventure', 'romantic movies', 'docuseries', 'stand-up comedy', 'music & musicals', 'tv dramas', 'thrillers', 'sports movies', 'anime series', 'korean tv shows', 'children & family movies', 'tv shows', 'international movies', 'classic & cult tv', 'tv mysteries', 'tv sci-fi & fantasy', 'tv horror', \"kids' tv\", 'british tv shows', 'anime features', 'romantic tv shows', 'classic movies', 'independent movies', 'movies', 'documentaries', 'spanish-language tv shows', 'faith & spirituality', 'reality tv', 'international tv shows', 'science & nature tv', 'dramas', 'sci-fi & fantasy']\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "print(dist_listed)\n",
    "print(len(dist_listed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t:tv action & adventure\n",
      "0\t:teen tv shows\n",
      "0\t:cult movies\n",
      "0\t:tv comedies\n",
      "0\t:horror movies\n",
      "0\t:tv thrillers\n",
      "0\t:crime tv shows\n",
      "0\t:lgbtq movies\n",
      "0\t:stand-up comedy & talk shows\n",
      "0\t:comedies\n",
      "0\t:action & adventure\n",
      "0\t:romantic movies\n",
      "0\t:docuseries\n",
      "0\t:stand-up comedy\n",
      "0\t:music & musicals\n",
      "0\t:tv dramas\n",
      "0\t:thrillers\n",
      "0\t:sports movies\n",
      "0\t:anime series\n",
      "1\t:korean tv shows\n",
      "0\t:children & family movies\n",
      "0\t:tv shows\n",
      "0\t:international movies\n",
      "0\t:classic & cult tv\n",
      "0\t:tv mysteries\n",
      "0\t:tv sci-fi & fantasy\n",
      "1\t:tv horror\n",
      "0\t:kids' tv\n",
      "0\t:british tv shows\n",
      "0\t:anime features\n",
      "0\t:romantic tv shows\n",
      "0\t:classic movies\n",
      "0\t:independent movies\n",
      "0\t:movies\n",
      "0\t:documentaries\n",
      "0\t:spanish-language tv shows\n",
      "0\t:faith & spirituality\n",
      "0\t:reality tv\n",
      "0\t:international tv shows\n",
      "0\t:science & nature tv\n",
      "0\t:dramas\n",
      "0\t:sci-fi & fantasy\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Scientists conduct research on sharks\"\n",
    "\n",
    "\n",
    "\n",
    "input_flags = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for flag,cat in zip(input_flags,dist_listed):\n",
    "    print(str(flag)+\"\\t:\"+cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X  !!!!!!!!!!!!!!!!!!!\"!!!!!!\"!!!!!!!!!!!!!!!\n",
      "Input :  Scientists conduct research on sharks\n",
      "Generated output :  Scientists conduct research on sharks!!!!!!!!!!!!!!!!!!!\"!!!!!!\"!!!!!!!!!!!!!!! -As her father nears the end of his life, he is contacted by their mother in an attempt to help them both faceTheWisekind family member Kirsten Johnson and David Kaitlyn (who was recently diagnosed with a brain tumor)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "X = torch.tensor(input_flags)\n",
    "\n",
    "\n",
    "encodings_dict = tokenizer('<|startoftext|>' + input_text + '<|endoftext|>',\n",
    "                                       max_length=max_length, padding=\"max_length\")\n",
    "Y =  torch.tensor(encodings_dict['input_ids'])    \n",
    "\n",
    "print(\"X \",tokenizer.decode(X, skip_special_tokens=True))\n",
    "XY = torch.cat((Y,X), 0)\n",
    "sample_input = torch.reshape(XY,(1,-1)).cuda()\n",
    "\n",
    "sample_outputs = model.generate(sample_input,no_repeat_ngram_size = 1,num_beams=20, num_return_sequences=2,max_new_tokens=50)\n",
    "print(\"Input : \",input_text)\n",
    "\n",
    "\n",
    "print(\"Generated output : \",tokenizer.decode(sample_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2534928701.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [46]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Generated output :  stranger!!!!!!!!!!!!!!!!!!!!!!\"!!!!!!!!!!!!!!!!!!!!'\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Input :  stranger\n",
    "Generated output :  stranger!!!!!!!!!!!!!!!!!!!!!!\"!!!!!!!!!!!!!!!!!!!!'\n",
    "As her grows his father nears, they come to help them both face the wrath of their captors and provide themselves with a great chance at its own peril. And there will be no one who has everything in my heart as it comes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tharhtet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b46ff7e5b8b7911cfa9955e23e477c53e63d207f4b9ab3253a6a5ac7336ecbe5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
